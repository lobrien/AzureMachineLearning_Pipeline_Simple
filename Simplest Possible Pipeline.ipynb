{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple 3-Step AzureML Pipeline (Dataprep, Training, and Evaluation)\n",
    "\n",
    "![Illustration of pipeline graph](./media/pipeline_graph.png)\n",
    "\n",
    "This demonstrates how you create a multistep AzureML pipeline using a series of `PythonScriptStep` objects. \n",
    "\n",
    "In this case, the calculation is extremely trivial: predicting Iris species using scikit-learn's Gaussian Naive Bayes. This pipeline could be solved (very quickly) using this code: \n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# These two lines become the data ingestion and dataprep steps \n",
    "df = pd.read_csv(\"iris.csv\", header=None)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,1:4], df.iloc[:,4:5], test_size=0.2, random_state=42)\n",
    "\n",
    "# These two lines become the training step\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# These two lines become the evaluation step\n",
    "prediction = model.predict(X_test)\n",
    "print(f'Accuracy: {accuracy_score(prediction, y_test):3f}')\n",
    "```\n",
    "\n",
    "The point of this notebook is to show the construction of the AzureML pipeline, not demonstrate any kind of complex machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary setup\n",
    "\n",
    "Import types used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment, Experiment, Workspace, Datastore, Dataset\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.data.datapath import DataPath\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Access the AzureML workspace (relies on `config.json` downloaded from workspace in same dir as notebook). \n",
    "\n",
    "* Retrieve the default datastore for the workspace. This is where the `Dataset` (permanent data) and temporary data will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "ds = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion\n",
    "\n",
    "Register the data as a `Dataset` within the ML workspace, if necessary. Relies, initially, on the presence of the iris dataset in the local `./data` dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_dataset_name = 'iris_baseline'\n",
    "\n",
    "if not baseline_dataset_name in Dataset.get_all(ws).keys() :\n",
    "    ds.upload(src_dir=\"./data/\", target_path='iris_data_baseline')\n",
    "    iris_dataset = Dataset.Tabular.from_delimited_files(DataPath(ds, 'iris_data_baseline/iris.csv'))\n",
    "    iris_dataset.register(ws, 'iris_baseline', description='Iris baseline data (w. header)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Resource & Python Environment\n",
    "\n",
    "For this super-easy problem, just use a CPU-based cluster and share the environment between pipeline steps. The curated environment `AzureML-Tutorial` happens to have `sklearn`, so that's why I chose it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_name = \"cpu-cluster\"\n",
    "vm_size = \"STANDARD_D2_V2\"\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('Found compute target: ' + compute_name)\n",
    "else:\n",
    "    print('Creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size,  # STANDARD_NC6 is GPU-enabled\n",
    "                                                                min_nodes=0,\n",
    "                                                                max_nodes=4)\n",
    "    # create the compute target\n",
    "    compute_target = ComputeTarget.create(\n",
    "        ws, compute_name, provisioning_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment.get(ws, \"AzureML-Tutorial\")\n",
    "\n",
    "runconfig = RunConfiguration()\n",
    "runconfig.target = compute_target\n",
    "runconfig.environment = env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation and augmentation step\n",
    "\n",
    "Now that the `Dataset` is registered, it's available for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dataset = Dataset.get_by_name(ws, 'iris_baseline')\n",
    "iris_dataset.take(3).to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_input = iris_dataset.as_named_input(\"iris_baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use `Dataset`s for initial input to a Pipeline.\n",
    "* Use `PipelineData` for temporary data that flows between pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_path = PipelineData(\"X_train_path\", datastore=ds)\n",
    "X_test_path = PipelineData(\"X_test_path\", datastore=ds)\n",
    "y_train_path = PipelineData(\"y_train_path\", datastore=ds)\n",
    "y_test_path = PipelineData(\"y_test_path\", datastore=ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataprep step:\n",
    "\n",
    "![image of dataprep step in graph](media/dataprep.png)\n",
    "\n",
    "Note how the `Dataset` input and `PipelineData` outputs are shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I set reuse to `False`, since part of this step is random selection of sets. ('Cept, of course, RANDOM_SEED is the same)\n",
    "dataprep_step = PythonScriptStep(\n",
    "    script_name = \"dataprep.py\",\n",
    "    arguments=[\n",
    "        \"--X_train_path\", X_train_path, \n",
    "        \"--y_train_path\", y_train_path,\n",
    "        \"--X_test_path\", X_test_path,\n",
    "        \"--y_test_path\", y_test_path],\n",
    "    inputs = [ds_input],\n",
    "    outputs = [X_train_path, y_train_path, X_test_path, y_test_path],\n",
    "    compute_target = compute_target,\n",
    "    source_directory=\"./src/dataprep\",\n",
    "    allow_reuse = False,\n",
    "    runconfig = runconfig\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The next step takes two outputs from the first step and writes the model to the `model_path` output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = PipelineData(\"model_path\", datastore=ds)\n",
    "\n",
    "training_step = PythonScriptStep(\n",
    "    script_name = \"train.py\",\n",
    "    arguments=[\n",
    "        \"--X_train_path\", X_train_path, \n",
    "        \"--y_train_path\", y_train_path,\n",
    "        \"--model_path\", model_path],\n",
    "    inputs = [X_train_path, y_train_path],\n",
    "    outputs = [model_path],\n",
    "    compute_target = compute_target,\n",
    "    source_directory=\"./src/train/\",\n",
    "    allow_reuse = True,\n",
    "    runconfig=runconfig\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Takes the `model_path` from the training step and the test data from the dataprep step. Internally, it reconstitutes the model, runs it against the test data, and writes something to the log (the child run's `70_driver_log.txt` file). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_step = PythonScriptStep(\n",
    "    script_name = \"evaluate.py\",\n",
    "    arguments=[\n",
    "        \"--model_path\", model_path,\n",
    "        \"--X_test_path\", X_test_path, \n",
    "        \"--y_test_path\", y_test_path],\n",
    "    inputs = [model_path, X_test_path, y_test_path],\n",
    "    compute_target = compute_target,\n",
    "    source_directory=\"./src/evaluate/\",\n",
    "    allow_reuse = True,\n",
    "    runconfig=runconfig\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pipeline\n",
    "\n",
    "* The source code associated with the individual steps is zipped up, uploaded. \n",
    "* The compute resource is allocated\n",
    "* A Docker image is built for it w. the necessary environment\n",
    "* The dependency graph for the pipeline is calculated\n",
    "* The steps execute, as necessary\n",
    "\n",
    "In this case, since I set `allow_reuse` to `False` in the first step, every run will cause a total rerun. The thing is that my very first step is where I do not just datapreparation, but the shuffling for the test/train split. That could be split into multiple steps if dataprep were an expensive operation. Or, if datapreparation manipulated both testing and training data, then you could have dataprep be one step and do the test/training split either at the beginning of the train step or as a separate step. \n",
    "\n",
    "I could imagine for instance, after the test/train split, you put the same data into two different training steps, which you directly compare in the evaluation split...\n",
    "\n",
    "But all of that goes beyond this simple example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the pipeline\n",
    "pipeline1 = Pipeline(workspace=ws, steps=[dataprep_step, training_step, eval_step])\n",
    "# Submit the pipeline to be run\n",
    "pipeline_run1 = Experiment(ws, 'Iris_SKLearn_Pipeline').submit(pipeline1)\n",
    "pipeline_run1.wait_for_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
